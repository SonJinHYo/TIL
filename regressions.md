# 회귀 종류



### 다항 회귀

- 선형 회귀를 이용하여 **비선형 데이터를 학습**하는 기법
- 비선형 데이터를 학습하는 데 선형 모델 사용을 가능하게 함



#### 다항 회귀 아이디어

- 특성 조합 활용
- 추가되는 특성은 기존의 특성값들의 거듭제곱, 특성값들 간의 곱으로 이뤄짐

- 특성 변수들의 다항식을 조합 특성으로 추가





## 규제 선형 모델에서의 회귀

- 자유도(degree of freedom) : 학습 모델 결정에 영향을 주는 요수(특성)의 수

  - 선형 회귀의 자유도 == 특성 수
  - 다항 회귀의 자유도 == 차수

- 규제(regularization) : 자유도 제한

  - 선형 회귀의 규제 == 가충치 역할 제한
  - 다항 회귀의 자유도 == 차수 줄이기

- 규제 적용 주의사항

  - 규제항은 훈련 과정에만 사용됨

  - 테스트 과정에는 다른 기준으로 성능을 평가

    - 훈련과정 - 비용 최소화 목표

    - 테스트과정 - 최종 목표에 따른 성능 평가

      예를 들어, 분류기의 경우 재현율/정밀도 기준으로 성능을 평가



### 릿지 회귀

- <img src="https://raw.githubusercontent.com/SonJinHYo/image_repo/main/image_server/image-20221004150139376.png" alt="image-20221004150139376" style="zoom:80%;" />

  - $$
    \alpha: 규제 강도를 지정하는 하이퍼파라미터. (\alpha=0 이면 선형회귀)  
    $$

  - $$
    \alpha가 커질수록 가중치의 역할이 줄어듬
    $$

  - 비용을 줄이기 위해 가중치를 작게 유지하는 방향으로 학습

  - 주의사항 : 훈련 세트에 대한 특성 스케일링 전처리 실행 후 적용



### 라쏘 회귀

- <img src="https://raw.githubusercontent.com/SonJinHYo/image_repo/main/image_server/image-20221011104200304.png" alt="image-20221011104200304" style="zoom:67%;" />

  - 알파는 릿지 회귀에서와 같은 역할

  - $$
    \theta i: 덜 중요한 특성을 무시하기 위해,  \theta 가 0에 수렴하도록 학습 유도
    $$

  - 주의사항 : theta0는 규제하지 않음





### 엘라스틱넷

- 릿지회귀와 라쏘회귀의 절충안 모델
- 

<img src="https://raw.githubusercontent.com/SonJinHYo/image_repo/main/image_server/image-20221011104540573.png" alt="image-20221011104540573" style="zoom:67%;" />

- r을 이용하여 릿지규제와 라쏘 규제를 적절하게 조절
- 릿지,라쏘,엘라스틱넷 선택 과정
  1. 릿지 규제를 기본으로 한다.
  2. 유용한 속성이 많지 않다고 판단된다면
     - 라쏘 규제나 엘라스틱넷을 활용한다
     - 불필요한 속성의 가중치를 0으로 만들어주기 때문
  3. 특성 수가 훈련 샘플 수보다 크거나 특성 몇 개가 강하게 연관되어 있는 경우
     - 라쏘 규제는 적절하지 않음
     - 이 때 엘라스틱넷 추천





#### 조기 종료 기법

- 배경 : 반복 훈련 과정 중에 모델이 훈련데이터에 점점 익숙해져서 과대적합 발생 가능성이 높아짐
- 반복훈련 종료 기준 : 검증데이터에 대한 손실이 줄어들다가 다시 커지는 순간
- 조기 종료: 검증 오차가 최소에 다다랐을 때, 반복 훈련을 멈추게 하는 기법
- 확률적 경사하강법, 미니배치 경사하강법의 경우
  - 검증 오차가 진동하는 경우가 대다수
  - 이 때는, 검증오차가 한동안 최솟값보다 높게 유지될 때 반복 훈련을 멈추고 검증 오차가 최소였을 때 모델 파라미터 확인







### 로지스틱 회귀

- 특성과 가중치를 곱한 값들을 더한 결과에 시그모이드 함수를 적용한 결과 이용
- <img src="https://raw.githubusercontent.com/SonJinHYo/image_repo/main/image_server/image-20221011105149139.png" alt="image-20221011105149139" style="zoom:67%;" />

- <img src="https://raw.githubusercontent.com/SonJinHYo/image_repo/main/image_server/image-20221011105441109.png" alt="image-20221011105441109" style="zoom:67%;" />
  - 목적 : 잘 맞추면 작은 손실값, 못 맞추면 큰 손실값을 준다.
    - p(확률)  -> 1 이면, log(p)-> 0에 가까운 음수값
    - p(확률)  -> 0 이면, log(p)-> -inf





### 소프트맥스 회귀

- 로지스틱 회귀 모델을 일반화하여 다중 클래스 분류를 지원하도록 한 회귀모델
- 다항 로지스틱 회귀라고도 불린다
- <img src="https://raw.githubusercontent.com/SonJinHYo/image_repo/main/image_server/image-20221011110050563.png" alt="image-20221011110050563" style="zoom:67%;" />
  - 비용함수에 대해 경사하강법 적용
  - 각 분류 클래스 k에 대한 적절한 가중치 벡터 theta k를 학습해 나가야함.
- k=2이면 로지스틱 회귀와 같음
